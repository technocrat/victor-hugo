<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Locus: Richard Careaga</title>
    <link>https://technocrat.rbind.io/post/</link>
    <description>Recent content in Posts on Locus: Richard Careaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://technocrat.rbind.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Newtonian Data Science</title>
      <link>https://technocrat.rbind.io/2018/08/04/newtonian-data-science/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://technocrat.rbind.io/2018/08/04/newtonian-data-science/</guid>
      <description>Most advanced machine learning and AI strike me as deterministic finite state machines with an error term. Properly tuned, the same input should produce much the same output every time. Like Newtonian physics, they can be complicated, but deconstructible into simple components. Like Newtonian physics, as well, they are highly accurate and reliable within their domain.
What happens when we start applying the same tools to complex systems? What are the complex systems of importance that refuse to yield identical results with identical inputs?</description>
    </item>
    
  </channel>
</rss>