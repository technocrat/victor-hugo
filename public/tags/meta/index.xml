<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>meta on Locus: Richard Careaga</title>
    <link>/tags/meta/</link>
    <description>Recent content in meta on Locus: Richard Careaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 May 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/meta/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Row collapsing</title>
      <link>/2019/05/03/duplicate-keys-for-multiple-logical-columns/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/duplicate-keys-for-multiple-logical-columns/</guid>
      <description>Untidy happens It’s not always possible to store everything in a tidy but humongous data store. So, we have things like SQL foreign keys.
Unfortunately, the database may not have been set up with future tidy in mind. Or, database access is only provided through CSV files.
Here’s an example of what can happen. We begin with some basic information, as tidy as we could wish with not too much effort.</description>
    </item>
    
    <item>
      <title>The compiler will tell you what the user cannot</title>
      <link>/2019/01/08/the-compiler-will-tell-you-what-the-user-cannot/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/08/the-compiler-will-tell-you-what-the-user-cannot/</guid>
      <description>The compiler will always tell you about source code errors that prevent compiling. It can’t advise you if your code solves the problem that it was supposed to solve, even if you are confident in what that problem is. But have a thought for the user who posed the problem, and keep in mind two famous quotations.
 Thus, programs must be executed for humans to read, only incidentially for machines to execute.</description>
    </item>
    
    <item>
      <title>Is this the original revised data or the revised revised data</title>
      <link>/2018/10/18/is-this-the-original-revised-data-or-the-revised-revised-data/</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/18/is-this-the-original-revised-data-or-the-revised-revised-data/</guid>
      <description>Keeping track of the provenance of data can be a challenge, especially when drawing on published sources. Keeping a record of the origin, the date accessed, the transformations applied (e.g., converting from .xls to cvs and converting character strings such as “$1,250,321.21” to floats or date strings to date objects), subsequent changes, who handled the data object and where it can be found in a repository are all things that enhance the analyst’s own ability to reproduce results.</description>
    </item>
    
    <item>
      <title>Cognitive Bias and the Data Scientist</title>
      <link>/2018/10/11/cognitive-bias-and-the-data-scientist/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/11/cognitive-bias-and-the-data-scientist/</guid>
      <description>As data scientists, we are not immune from cognitive bias. We do a lot to minimize it, we may even try to quantify it. But it is a part of our protoplasm and, especially, the protoplasm of our untrained clients.
A good place to start is the Monty Hall Problem. The pre-historic Let’s Make a Deal game show on broadcast television featured a host, Monty Hall, and contestants who were offered the chance to win fabulous prizes if they picked the right curtain, 1, 2, or 3.</description>
    </item>
    
    <item>
      <title>Cognitive styles of data science</title>
      <link>/2018/09/10/cognitive-styles-of-data-science/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/10/cognitive-styles-of-data-science/</guid>
      <description>Yihui Xie of RMarkdown fame has a long, thoughtful post (https://goo.gl/WjA7t8) on a debate for and against working in various types of interactive notebooks, such as Juypter for the Python platform, IDEs like RStudio in its so-called “notebook mode” and the old school heavy code commenting, among other approaches. Although that’s the primary subject, he weaves in some astute observations of the differences among “data scientists” based on whether their perspective grows from data origins or is aimed at ultimately producing software.</description>
    </item>
    
    <item>
      <title>AI and the Oracle at Delphi</title>
      <link>/2018/08/30/ai-and-the-oracle-at-delphi/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/30/ai-and-the-oracle-at-delphi/</guid>
      <description>I attended a nice Meetup last night, and one of the speakers quoted Turing’s description of a program able to modify its own instructions as a good description of artificial intelligence (AI) and another had a nifty typology relating all the standard tools in data science to their role in AI. This set me to thinking about the standards to which AI should be held.
I began a thought experiment:</description>
    </item>
    
    <item>
      <title>The plural of data science is not formulae</title>
      <link>/2018/08/14/the-plural-of-data-science-is-not-formulae/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/14/the-plural-of-data-science-is-not-formulae/</guid>
      <description>The WSJ report of academic research on career hot streaks has snagged a lot of LinkedIn eyeballs today (Career ‘hot steaks’ are real 1h ago • 3,526 readers).
Reading the paper (https://goo.gl/2Mwqjn), you can see that it is an observational study of a convenience sample (these were the people about whom data were available, in the thousands or tens of thousands, an impressive collection).
Two mysteries: Why were the data not stratified into a training set and a test set?</description>
    </item>
    
    <item>
      <title>Newtonian Data Sciences</title>
      <link>/2018/08/06/newtonian-data-sciences/</link>
      <pubDate>Mon, 06 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/06/newtonian-data-sciences/</guid>
      <description>Advanced machine learning and AI strike me as deterministic finite state machines with an error term. Properly tuned, the same input should produce much the same output every time. Like Newtonian physics, they can be complicated, but deconstructible into simple components. Like Newtonian physics, as well, they are highly accurate and reliable within their domain.
What happens when we start applying the same tools to complex systems? What are the complex systems of importance that refuse to yield identical results with identical inputs?</description>
    </item>
    
  </channel>
</rss>