<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Locus: Richard Careaga</title>
    <link>/categories/data-science/</link>
    <description>Recent content in Data Science on Locus: Richard Careaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cognitive styles of data science</title>
      <link>/2018/09/10/cognitive-styles-of-data-science/</link>
      <pubDate>Mon, 10 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/10/cognitive-styles-of-data-science/</guid>
      <description>Yihui Xie of RMarkdown fame has a long, thoughtful post (https://goo.gl/WjA7t8) on a debate for and against working in various types of interactive notebooks, such as Juypter for the Python platform, IDEs like RStudio in its so-called &amp;ldquo;notebook mode&amp;rdquo; and the old school heavy code commenting, among other approaches. Although that&amp;rsquo;s the primary subject, he weaves in some astute observations of the differences among &amp;ldquo;data scientists&amp;rdquo; based on whether their perspective grows from data origins or is aimed at ultimately producing software.</description>
    </item>
    
    <item>
      <title>Deboning simple linear regression output in TensorFlow</title>
      <link>/2018/09/04/deboning-simple-linear-regression-output-in-tensorflow/</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/04/deboning-simple-linear-regression-output-in-tensorflow/</guid>
      <description>A simple linear regression example in TensorFlow&amp;trade; is given at (https://www.tensorflow.org/tutorials/keras/basic_regression). By default, it produces none of the outputs of linear regression, other than MAE or MSE, on request through a print statement.
Testing set Mean Abs Error: $2788.86  Time spent searching the documentation turned up no references to &amp;ldquo;intercept&amp;rdquo;, &amp;ldquo;coefficient&amp;rdquo;, &amp;ldquo;F-statistic&amp;rdquo;, &amp;ldquo;t-value,&amp;rdquo; or &amp;ldquo;p-value.&amp;rdquo;
I am only beginning to dip my toes into TensorFlow&amp;trade;. I suspect that it exists not so much as a suite of modeling tools so much as a platform to express them in n-dimensional space very efficiently.</description>
    </item>
    
    <item>
      <title>The Unbearable Lightness of Coloring Maps</title>
      <link>/2018/08/31/the-unbearable-lightness-of-coloring-maps/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/31/the-unbearable-lightness-of-coloring-maps/</guid>
      <description>The Federal Bureau of Investigation (FBI) has lead responsibilities for protecting the United States against domestic crimes initiated abroad. These responsibilities include protection against &amp;ldquo;covert actions by foreign governments to influence U.S. political sentiment or public discourse&amp;rdquo; (https://www.fbi.gov/investigate/counterintelligence/foreign-influence). It has illustrated its mission description with a world map that appears to classify nations by the threats they pose.
The map has no description or legend, but uses several shades of blue to distinguish among countries.</description>
    </item>
    
    <item>
      <title>AI and the Oracle at Delphi</title>
      <link>/2018/08/30/ai-and-the-oracle-at-delphi/</link>
      <pubDate>Thu, 30 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/30/ai-and-the-oracle-at-delphi/</guid>
      <description>I attended a nice Meetup last night, and one of the speakers quoted Turing&amp;rsquo;s description of a program able to modify its own instructions as a good description of artificial intelligence (AI) and another had a nifty typology relating all the standard tools in data science to their role in AI. This set me to thinking about the standards to which AI should be held.
I began a thought experiment:</description>
    </item>
    
    <item>
      <title>How to debone linear regression output in Python</title>
      <link>/2018/08/29/how-to-debone-linear-regression-output-in-python/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/29/how-to-debone-linear-regression-output-in-python/</guid>
      <description>This post makes a trio with two most recent blogs on the same subject for R and Excel, this time in Python usking the sklearn package.
The example given in the official documentation (https://goo.gl/GGmUKK) is somewhat sketchy and uses a different dataset than the previous two examples. Although the example dataset is different, my main purpose is to illustrate the default output.
 OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.</description>
    </item>
    
    <item>
      <title>Deboning linear regression output in Excel</title>
      <link>/2018/08/28/deboning-linear-regression-output-in-excel/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/28/deboning-linear-regression-output-in-excel/</guid>
      <description>The other day (https://goo.gl/1W11Zu), I outlined interpretation of the output of a multiple linear regression of data on Seattle area housing prices (https://www.kaggle.com/harlfoxem/housesalesprediction?login=true), which provides a convenient way to illustrate the usual output of a multiple linear regression model output in R. This is a 21K dataset with 19 variables on housing characteristics and sales price. It’s a cruddy model, used solely to pick apart the different data presented. Today, it’s Excel’s turn.</description>
    </item>
    
    <item>
      <title>How to debone the output of a linear regression model</title>
      <link>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</guid>
      <description>I was recently puzzling over an academic paper in the social sciences with a multiple linear regression model that seemed off. (Communication with the authors educated me on considerations that resolved my concerns.)
During the course of my puzzling, I compared what I was seeing as model output in the paper and what I expected to see based on experience. The embarrassing revelation dawned that I didn’t actually know what all those components meant.</description>
    </item>
    
    <item>
      <title>The plural of data science is not formulae</title>
      <link>/2018/08/14/the-plural-of-data-science-is-not-formulae/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/14/the-plural-of-data-science-is-not-formulae/</guid>
      <description>The WSJ report of academic research on career hot streaks has snagged a lot of LinkedIn eyeballs today (Career &amp;lsquo;hot steaks&amp;rsquo; are real 1h ago • 3,526 readers).
Reading the paper (https://goo.gl/2Mwqjn), you can see that it is an observational study of a convenience sample (these were the people about whom data were available, in the thousands or tens of thousands, an impressive collection).
Two mysteries: Why were the data not stratified into a training set and a test set?</description>
    </item>
    
    <item>
      <title>Nathan Yau on the challenges of thematic maps</title>
      <link>/2018/08/09/nathan-yau-on-the-challenges-of-thematic-maps/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/09/nathan-yau-on-the-challenges-of-thematic-maps/</guid>
      <description>If you are a paying member of Flowing Data (well worth the modest subscription), you will have received his email, reproduced at a paywalled portion of the site.
He gives a detailed critique of a precinct-by-precinct map of the 2016 presidential election, nationwide, showing the number of votes for Trump and Clinton and the nearest precinct in which the opposing candidate was in the majority.
He compares what the map purports to show with what it actually does show, which is the raw total of votes (ho-hum, we&amp;rsquo;ve known that for a long time) and the percentages.</description>
    </item>
    
    <item>
      <title>Newtonian Data Science</title>
      <link>/2018/08/04/newtonian-data-science/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/04/newtonian-data-science/</guid>
      <description>Most advanced machine learning and AI strike me as deterministic finite state machines with an error term. Properly tuned, the same input should produce much the same output every time. Like Newtonian physics, they can be complicated, but deconstructible into simple components. Like Newtonian physics, as well, they are highly accurate and reliable within their domain.
What happens when we start applying the same tools to complex systems? What are the complex systems of importance that refuse to yield identical results with identical inputs?</description>
    </item>
    
  </channel>
</rss>