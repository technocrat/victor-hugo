<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Locus: Richard Careaga</title>
    <link>/categories/data-science/</link>
    <description>Recent content in Data Science on Locus: Richard Careaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to debone linear regression output in Python</title>
      <link>/2018/08/29/how-to-debone-linear-regression-output-in-python/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/29/how-to-debone-linear-regression-output-in-python/</guid>
      <description>This post makes a trio with two most recent blogs on the same subject for R and Excel, this time in Python usking the sklearn package.
The example given in the official documentation (https://goo.gl/GGmUKK) is somewhat sketchy and uses a different dataset than the previous two examples. Although the example dataset is different, my main purpose is to illustrate the default output.
 OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.</description>
    </item>
    
    <item>
      <title>Deboning linear regression output in Excel</title>
      <link>/2018/08/28/deboning-linear-regression-output-in-excel/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/28/deboning-linear-regression-output-in-excel/</guid>
      <description>The other day (https://goo.gl/1W11Zu), I outlined interpretation of the output of a multiple linear regression of data on [Seattle area housing prices (https://www.kaggle.com/harlfoxem/housesalesprediction?login=true), which provides a convenient way to illustrate the usual output of a multiple linear regression model output in R. This is a 21K dataset with 19 variables on housing characteristics and sales price. It’s a cruddy model, used solely to pick apart the different data presented. Today, it’s Excel’s turn.</description>
    </item>
    
    <item>
      <title>How to debone the output of a linear regression model</title>
      <link>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</guid>
      <description>I was recently puzzling over an academic paper in the social sciences with a multiple linear regression model that seemed off. (Communication with the authors educated me on considerations that resolved my concerns.)
During the course of my puzzling, I compared what I was seeing as model output in the paper and what I expected to see based on experience. The embarrassing revelation dawned that I didn’t actually know what all those components meant.</description>
    </item>
    
    <item>
      <title>The plural of data science is not formulae</title>
      <link>/2018/08/14/the-plural-of-data-science-is-not-formulae/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/14/the-plural-of-data-science-is-not-formulae/</guid>
      <description>The WSJ report of academic research on career hot streaks has snagged a lot of LinkedIn eyeballs today (Career &amp;lsquo;hot steaks&amp;rsquo; are real 1h ago • 3,526 readers).
Reading the paper (https://goo.gl/2Mwqjn), you can see that it is an observational study of a convenience sample (these were the people about whom data were available, in the thousands or tens of thousands, an impressive collection).
Two mysteries: Why were the data not stratified into a training set and a test set?</description>
    </item>
    
    <item>
      <title>Nathan Yau on the challenges of thematic maps</title>
      <link>/2018/08/09/nathan-yau-on-the-challenges-of-thematic-maps/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/09/nathan-yau-on-the-challenges-of-thematic-maps/</guid>
      <description>If you are a paying member of Flowing Data (well worth the modest subscription), you will have received his email, reproduced at a paywalled portion of the site.
He gives a detailed critique of a precinct-by-precinct map of the 2016 presidential election, nationwide, showing the number of votes for Trump and Clinton and the nearest precinct in which the opposing candidate was in the majority.
He compares what the map purports to show with what it actually does show, which is the raw total of votes (ho-hum, we&amp;rsquo;ve known that for a long time) and the percentages.</description>
    </item>
    
    <item>
      <title>Newtonian Data Science</title>
      <link>/2018/08/04/newtonian-data-science/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/04/newtonian-data-science/</guid>
      <description>Most advanced machine learning and AI strike me as deterministic finite state machines with an error term. Properly tuned, the same input should produce much the same output every time. Like Newtonian physics, they can be complicated, but deconstructible into simple components. Like Newtonian physics, as well, they are highly accurate and reliable within their domain.
What happens when we start applying the same tools to complex systems? What are the complex systems of importance that refuse to yield identical results with identical inputs?</description>
    </item>
    
  </channel>
</rss>