<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Locus: Richard Careaga</title>
    <link>/tags/statistics/</link>
    <description>Recent content in statistics on Locus: Richard Careaga</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 27 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>n?</title>
      <link>/2018/10/27/n/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/27/n/</guid>
      <description>Got n?
There was a 100% increase in 1-hour parking meter costs this year!
(Imagined conversation in a small town.)
It appears to be hardwired into our wetware to forget to look for the underlying integers. You might ask, for example from what to what? and learn that it went up from a nickle to a dime.
This is a manifestation what Daniel Kahneman, author of Thinking Fast, and Slow (2011, Ferrar, Straus and Giroux, New York) calls The Law of Small Numbers.</description>
    </item>
    
    <item>
      <title>Ordinary Least Squares Linear Regression Walkthrough for Beginners</title>
      <link>/2018/09/30/ordinary-least-squares-linear-regression-walkthrough-for-beginners/</link>
      <pubDate>Sun, 30 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/30/ordinary-least-squares-linear-regression-walkthrough-for-beginners/</guid>
      <description>Ordinary Least Squares Linear Regression Ordinary linear least squares regression is one of the first statistical methods that anyone learns who is trying to understand how two sets of numbers relate. It has an undeserved reputation of being able to foretell the future, but it is mathematically tractable, simple to apply, and often yields either directly usable results or signals the need for more sophisticated tools.
The idea is simple enough, find the line that passes through points in Cartesian (x,y 2-dimensional space) that minimizes the total distance (another over-simplification) between each point and the line.</description>
    </item>
    
    <item>
      <title>Deboning linear regression output in Excel</title>
      <link>/2018/09/18/deboning-linear-regression-in-excel/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/18/deboning-linear-regression-in-excel/</guid>
      <description>A while back (https://goo.gl/1W11Zu), I outlined interpretation of the output of a multiple linear regression of data on Seattle area housing prices (https://www.kaggle.com/harlfoxem/housesalesprediction?login=true), which provides a convenient way to illustrate the usual output of a multiple linear regression model output in R. This is a 21K dataset with 19 variables on housing characteristics and sales price. It’s a cruddy model, used solely to pick apart the different data presented. Today, it’s Excel’s turn.</description>
    </item>
    
    <item>
      <title>Deboning simple linear regression output in TensorFlow</title>
      <link>/2018/09/04/deboning-simple-linear-regression-output-in-tensorflow/</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/04/deboning-simple-linear-regression-output-in-tensorflow/</guid>
      <description>A simple linear regression example in TensorFlow(TM) is given at (https://www.tensorflow.org/tutorials/keras/basic_regression). By default, it produces none of the outputs of linear regression, other than MAE or MSE, on request through a print statement.
Testing set Mean Abs Error: $2788.86 Time spent searching the documentation turned up no references to “intercept”, “coefficient”, “F-statistic”, “t-value,” or “p-value.”
I am only beginning to dip my toes into TensorFlow(TM). I suspect that it exists not so much as a suite of modeling tools so much as a platform to express them in n-dimensional space very efficiently.</description>
    </item>
    
    <item>
      <title>How to debone linear regression output in Python</title>
      <link>/2018/08/29/how-to-debone-linear-regression-output-in-python/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/29/how-to-debone-linear-regression-output-in-python/</guid>
      <description>This post makes a trio with two most recent blogs on the same subject for R and Excel, this time in Python usking the sklearn package.
The example given in the official documentation (https://goo.gl/GGmUKK) is somewhat sketchy and uses a different dataset than the previous two examples. Although the example dataset is different, my main purpose is to illustrate the default output.
 OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.</description>
    </item>
    
    <item>
      <title>How to debone the output of a linear regression model</title>
      <link>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/27/how-to-debone-the-output-of-a-linear-regression-model/</guid>
      <description>I was recently puzzling over an academic paper in the social sciences with a multiple linear regression model that seemed off. (Communication with the authors educated me on considerations that resolved my concerns.)
During the course of my puzzling, I compared what I was seeing as model output in the paper and what I expected to see based on experience. The embarrassing revelation dawned that I didn’t actually know what all those components meant.</description>
    </item>
    
  </channel>
</rss>