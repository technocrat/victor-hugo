---
title: How to debone linear regression output in Python
author: Richard Careaga
date: '2018-08-29'
slug: how-to-debone-linear-regression-output-in-python
categories:
  - Data Science
tags:
  - statistics
  - python
---



<p>This post makes a trio with two most recent blogs on the same subject for <em>R</em> and <em>Excel,</em> this time in <em>Python</em> usking the <em>sklearn</em> package.</p>
<p>The example given in the official documentation (<a href="https://goo.gl/GGmUKK" class="uri">https://goo.gl/GGmUKK</a>) is somewhat sketchy and uses a different dataset than the previous two examples. Although the example dataset is different, my main purpose is to illustrate the default output.</p>
<pre><code>                        OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.518
Model:                            OLS   Adj. R-squared:                  0.507
Method:                 Least Squares   F-statistic:                     46.27
Date:                Wed, 29 Aug 2018   Prob (F-statistic):           3.83e-62
Time:                        08:39:07   Log-Likelihood:                -2386.0
No. Observations:                 442   AIC:                             4794.
Df Residuals:                     431   BIC:                             4839.
Df Model:                          10
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const        152.1335      2.576     59.061      0.000     147.071     157.196
x1           -10.0122     59.749     -0.168      0.867    -127.448     107.424
x2          -239.8191     61.222     -3.917      0.000    -360.151    -119.488
x3           519.8398     66.534      7.813      0.000     389.069     650.610
x4           324.3904     65.422      4.958      0.000     195.805     452.976
x5          -792.1842    416.684     -1.901      0.058   -1611.169      26.801
x6           476.7458    339.035      1.406      0.160    -189.621    1143.113
x7           101.0446    212.533      0.475      0.635    -316.685     518.774
x8           177.0642    161.476      1.097      0.273    -140.313     494.442
x9           751.2793    171.902      4.370      0.000     413.409    1089.150
x10           67.6254     65.984      1.025      0.306     -62.065     197.316
==============================================================================
Omnibus:                        1.506   Durbin-Watson:                   2.029
Prob(Omnibus):                  0.471   Jarque-Bera (JB):                1.404
Skew:                           0.017   Prob(JB):                        0.496
Kurtosis:                       2.726   Cond. No.                         227.
==============================================================================</code></pre>
<p>Source: User JARH on Stackoveflow (<a href="https://goo.gl/vZ3UWP" class="uri">https://goo.gl/vZ3UWP</a>), using the same dataset as the official documentation.</p>
<p>The output is divided into three sections.</p>
<ol style="list-style-type: decimal">
<li>OLS REGRESSION RESULTS lacks the “call,” identifying the dependent and independent variables. It contains less information on residuals than <strong>R</strong> and adds some additional information. The date/time stamp is a gift during the exploratory data phase before the analyst has started more formal procedures.</li>
</ol>
<p>Four additional substantive fields are added</p>
<ul>
<li>Covariance Type is a useful reminder whether the the requirements for a reliable linear regression are satisfied totally or not (“nonrobust”).</li>
<li>Log-likelihood specifies which measure was applied to deal with the role of error terms</li>
<li>AIC – the Akalike information criterion – is a “score” to assess the quality of competing models</li>
<li>BIC – the Baysian inforation criterion – performs a similar role</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><p>The second section, below the “===============” line is similar to <strong>R</strong> and <strong>Excel</strong> in providing coefficients and standard error. Like Excel, it provides a confidence interval (but in standard 92.5/97.5 form for the 95% confidence interval) that has to be run separately in R. Unlike the other two programs it omits the convenient asterisk coding of significance levels.</p></li>
<li><p>The third section, below the “================” has information omitted from both <strong>R</strong> and <strong>Excel</strong> or in a different form.</p></li>
</ol>
<ul>
<li>The Omnibus test is the F statistic from analysis of variance (ANOVA), included in Excel and separatly available in R. Skew and kurtosis describe asymmetries in the distribution curve. Durbin-Watson tests for autocorrelation. It is separately available in R; I haven’t checked in Excel. Jarque-Bera tests whether the data is normally distributed. It is separately available in R; I haven’t checked in Excel. The condition number is a measure of collinearity among the independent variables, which can have the effect of overemmphasizing redundant variables. It is separately available in R; I haven’t checked in Excel.</li>
</ul>
<p>As sometimes said of economists, you can lay them end-to-end, and they would still point in different directions. Sklearn’s community of designers make different choices from those of R and the proprietary team of Microsoft Excel. Which is better or worse is a matter of personal taste, so long as each provides some functionality to produce the same output separately from the default for linear regression output.</p>
<p>If you don’t regularly use all three, I hope these guides have been useful.</p>
